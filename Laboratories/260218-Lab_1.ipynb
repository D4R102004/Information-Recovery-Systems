{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3f90b8",
   "metadata": {},
   "source": [
    "# Laboratorio #1: Indexaci√≥n\n",
    "\n",
    "En esta clase pr√°ctica aprenderemos c√≥mo funcionan los sistemas de b√∫squeda de informaci√≥n mediante la implementaci√≥n de un √≠ndice invertido. Un √≠ndice invertido es una estructura de datos fundamental que permite realizar b√∫squedas eficientes en grandes colecciones de documentos, similar a c√≥mo funciona un motor de b√∫squeda web.\n",
    "\n",
    "Durante la pr√°ctica, construiremos paso a paso:\n",
    "- Una estructura para representar documentos\n",
    "- Un pipeline de preprocesamiento de texto\n",
    "- Un √≠ndice invertido b√°sico con posting lists\n",
    "- Funcionalidad de b√∫squeda\n",
    "\n",
    "Este sistema nos permitir√° entender los conceptos fundamentales detr√°s de herramientas como Google, Elasticsearch, o cualquier motor de b√∫squeda moderno.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6c666",
   "metadata": {},
   "source": [
    "## Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96454c-c7ae-49af-94d5-29dc9e6801dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U spacy numpy matplotlib\n",
    "%pip install ir-datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "download(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e7ab85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import spacy\n",
    "\n",
    "# Configuraci√≥n de visualizaciones\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02691ec9",
   "metadata": {},
   "source": [
    "## Obtenci√≥n del dataset a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341abd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "cranfield_dataset = ir_datasets.load(\"cranfield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce275c67",
   "metadata": {},
   "source": [
    "## Estructura de Documentos\n",
    "\n",
    "Definimos la estructura b√°sica de un documento en nuestro sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Estructura b√°sica de un documento en el sistema DocuSearch\n",
    "    \n",
    "    Args:\n",
    "        id: str. Identificador √∫nico del documento\n",
    "        title: str. T√≠tulo del documento (opcional)\n",
    "        content: str. Contenido principal del documento\n",
    "        author: str. Autor del documento (opcional)\n",
    "        date: str. Fecha de publicaci√≥n (opcional)\n",
    "        category: str. Categor√≠a del documento (opcional)\n",
    "        metadata: Dict. Metadatos adicionales (opcional)\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    title: str = \"\"\n",
    "    content: str\n",
    "    author: str = \"\"\n",
    "    date: str = \"\"\n",
    "    category: str = \"\"\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "    \n",
    "    def get_full_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Retorna el texto completo del documento (t√≠tulo + contenido)\n",
    "        \n",
    "        Returns:\n",
    "            str. Texto completo combinando t√≠tulo y contenido\n",
    "        \"\"\"\n",
    "        return f\"{self.title} {self.content}\"\n",
    "    \n",
    "    def get_word_count(self) -> int:\n",
    "        \"\"\"\n",
    "        Retorna el n√∫mero de palabras en el documento\n",
    "        \n",
    "        Returns:\n",
    "            int. N√∫mero total de palabras\n",
    "        \"\"\"\n",
    "        return len(self.get_full_text().split())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Doc[{self.id}]: {self.title[:50]}{'...' if len(self.title) > 50 else ''}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278555f",
   "metadata": {},
   "source": [
    "## Pipeline de Preprocesamiento\n",
    "\n",
    "### ¬øQu√© es la tokenizaci√≥n y por qu√© la necesitamos?\n",
    "\n",
    "La **tokenizaci√≥n** es el proceso de dividir un texto en unidades m√°s peque√±as llamadas \"tokens\" (generalmente palabras). Es un paso fundamental en el procesamiento de lenguaje natural porque:\n",
    "\n",
    "1. **Normalizaci√≥n**: Convierte el texto a una forma est√°ndar (min√∫sculas, elimina puntuaci√≥n)\n",
    "2. **Limpieza**: Remueve elementos innecesarios como stopwords (\"el\", \"la\", \"de\", etc.)\n",
    "3. **Reducci√≥n**: Mediante lematizaci√≥n, reduce palabras a su forma base (ej: \"corriendo\" ‚Üí \"correr\")\n",
    "4. **Eficiencia**: Permite crear √≠ndices m√°s compactos y b√∫squedas m√°s r√°pidas\n",
    "\n",
    "Sin tokenizaci√≥n, no podr√≠amos comparar eficientemente el texto de b√∫squeda con los documentos indexados.\n",
    "\n",
    "Implementamos las etapas de procesamiento de texto usando SpaCy: tokenizaci√≥n, lematizaci√≥n y filtrado b√°sico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b7f17-32dd-4741-862a-cca212634170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyDocumentProcessor:\n",
    "    \"\"\"\n",
    "    Procesador de documentos usando SpaCy para tokenizaci√≥n y normalizaci√≥n\n",
    "    \n",
    "    Args:\n",
    "        remove_stopwords: bool. Si True, elimina stopwords del texto\n",
    "        min_word_length: int. Longitud m√≠nima de palabras a conservar\n",
    "        use_lemma: bool. Si True, usa lematizaci√≥n\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 remove_stopwords=True,\n",
    "                 min_word_length=2,\n",
    "                 use_lemma=True):\n",
    "        \n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.min_word_length = min_word_length\n",
    "        self.use_lemma = use_lemma\n",
    "\n",
    "    def process_text(self, text: str):\n",
    "        \"\"\"\n",
    "        Procesa un texto y retorna lista de t√©rminos normalizados\n",
    "        \n",
    "        Args:\n",
    "            text: str. Texto a procesar\n",
    "            \n",
    "        Returns:\n",
    "            list. Lista de t√©rminos procesados\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        terms = []\n",
    "\n",
    "        for token in doc:\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            if self.remove_stopwords and token.is_stop:\n",
    "                continue\n",
    "            \n",
    "            term = token.lemma_ if self.use_lemma else token.text\n",
    "            term = term.lower()\n",
    "\n",
    "            if len(term) >= self.min_word_length:\n",
    "                terms.append(term)\n",
    "\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ab5c9",
   "metadata": {},
   "source": [
    "## √çndice Invertido Simple con Posting Lists\n",
    "\n",
    "### EJERCICIOS DE LA CLASE\n",
    "\n",
    "En esta secci√≥n implementar√°s las funciones principales del √≠ndice invertido:\n",
    "\n",
    "**EJERCICIO 1**: Completar la funci√≥n `add_document()`\n",
    "\n",
    "**EJERCICIO 2**: Completar la funci√≥n `build_index()`\n",
    "\n",
    "Busca los comentarios `# COMPLETAR AQU√ç` para saber d√≥nde escribir el c√≥digo.\n",
    "\n",
    "---\n",
    "\n",
    "Implementamos la estructura b√°sica del √≠ndice invertido con posting lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInvertedIndex:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n b√°sica de un √≠ndice invertido con posting lists\n",
    "    \n",
    "    La estructura es: {t√©rmino: {doc_id: frecuencia}}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el √≠ndice invertido\n",
    "        \"\"\"\n",
    "        self.index = defaultdict(dict)\n",
    "        self.documents = {}           # doc_id -> Document object\n",
    "        self.processor = SpacyDocumentProcessor()\n",
    "        self.vocab_size = 0\n",
    "        self.total_documents = 0\n",
    "        \n",
    "    \n",
    "    # ============================================================================\n",
    "    # EJERCICIO 1: COMPLETAR LA FUNCI√ìN add_document()\n",
    "    # ============================================================================\n",
    "    def add_document(self, document: Document) -> None:\n",
    "        \"\"\"\n",
    "        A√±ade un documento al √≠ndice creando las posting lists\n",
    "        \n",
    "        Args:\n",
    "            document: Document. Documento a indexar\n",
    "        \"\"\"\n",
    "        # ---------------------- COMPLETAR AQU√ç ----------------------\n",
    "        # Paso 1: Procesar el texto del documento usando self.processor.process_text()\n",
    "        # Asignar el resultado a la variable 'terms'\n",
    "        terms = None  # REEMPLAZAR: obtener t√©rminos del documento\n",
    "        \n",
    "        # Paso 2: Contar las frecuencias de cada t√©rmino\n",
    "        # Usar Counter() para contar cu√°ntas veces aparece cada t√©rmino\n",
    "        term_counts = None  # REEMPLAZAR: contar frecuencias\n",
    "        \n",
    "        # Paso 3: A√±adir t√©rminos al √≠ndice\n",
    "        # Para cada t√©rmino y su frecuencia, agregar al √≠ndice invertido\n",
    "        # self.index[term][document.id] = frecuencia\n",
    "        for term, freq in ...:  # REEMPLAZAR: iterar sobre term_counts\n",
    "            pass  # REEMPLAZAR: agregar al √≠ndice\n",
    "        # ------------------------------------------------------------\n",
    "        \n",
    "        # Almacenar documento\n",
    "        self.documents[document.id] = document\n",
    "        \n",
    "        # Actualizar estad√≠sticas\n",
    "        self.vocab_size = len(self.index)\n",
    "        self.total_documents = len(self.documents)\n",
    "        \n",
    "    \n",
    "    # ============================================================================\n",
    "    # EJERCICIO 2: COMPLETAR LA FUNCI√ìN build_index()\n",
    "    # ============================================================================\n",
    "    def build_index(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Construye el √≠ndice desde una colecci√≥n de documentos\n",
    "        \n",
    "        Args:\n",
    "            documents: List[Document]. Lista de documentos a indexar\n",
    "        \"\"\"\n",
    "        print(f\"Iniciando indexaci√≥n de {len(documents)} documentos...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            # ---------------------- COMPLETAR AQU√ç ----------------------\n",
    "            # Llamar a add_document() para cada documento\n",
    "            pass  # REEMPLAZAR: agregar documento al √≠ndice\n",
    "            # ------------------------------------------------------------\n",
    "            \n",
    "            if i % 500 == 0:  # Progreso cada 500 documentos\n",
    "                print(f\"   Progreso: {i}/{len(documents)} documentos procesados\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\nIndexaci√≥n completada en {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"   Documentos: {self.total_documents}\")\n",
    "        print(f\"   Vocabulario: {self.vocab_size} t√©rminos √∫nicos\")\n",
    "    \n",
    "    def search(self, query: str) -> List[Tuple[str, Document]]:\n",
    "        \"\"\"\n",
    "        Realiza b√∫squeda simple en el √≠ndice usando posting lists\n",
    "        \n",
    "        Args:\n",
    "            query: str. Consulta de b√∫squeda\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, Document]]. Lista de tuplas (doc_id, document) con resultados\n",
    "        \"\"\"\n",
    "        # Procesar la consulta\n",
    "        query_terms = self.processor.process_text(query)\n",
    "        \n",
    "        if not query_terms:\n",
    "            print(\"La consulta no contiene t√©rminos v√°lidos despu√©s del procesamiento\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Buscando t√©rminos: {query_terms}\")\n",
    "        \n",
    "        # Encontrar documentos que contienen TODOS los t√©rminos (AND)\n",
    "        result_docs = None\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.index:\n",
    "                term_docs = set(self.index[term].keys())\n",
    "                if result_docs is None:\n",
    "                    result_docs = term_docs\n",
    "                else:\n",
    "                    result_docs &= term_docs\n",
    "            else:\n",
    "                print(f\"   T√©rmino '{term}' no encontrado en el √≠ndice\")\n",
    "                return []  # Si alg√∫n t√©rmino no existe, no hay resultados\n",
    "        \n",
    "        if result_docs is None:\n",
    "            return []\n",
    "        \n",
    "        # Convertir IDs a objetos Document\n",
    "        results = [(doc_id, self.documents[doc_id]) for doc_id in result_docs]\n",
    "        \n",
    "        print(f\"Encontrados {len(results)} documentos\")\n",
    "        return results\n",
    "    \n",
    "    def get_term_frequency(self, term: str) -> int:\n",
    "        \"\"\"\n",
    "        Retorna el n√∫mero de documentos que contienen un t√©rmino\n",
    "        \n",
    "        Args:\n",
    "            term: str. T√©rmino a buscar\n",
    "            \n",
    "        Returns:\n",
    "            int. N√∫mero de documentos que contienen el t√©rmino\n",
    "        \"\"\"\n",
    "        return len(self.index.get(term, set()))\n",
    "    \n",
    "    def get_document(self, doc_id: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Recupera un documento por su ID\n",
    "        \n",
    "        Args:\n",
    "            doc_id: str. ID del documento\n",
    "            \n",
    "        Returns:\n",
    "            Document. Objeto Document o None si no existe\n",
    "        \"\"\"\n",
    "        return self.documents.get(doc_id)\n",
    "    \n",
    "    def get_vocabulary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retorna lista de todos los t√©rminos en el vocabulario\n",
    "        \n",
    "        Returns:\n",
    "            List[str]. Lista de t√©rminos √∫nicos\n",
    "        \"\"\"\n",
    "        return list(self.index.keys())\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Retorna estad√≠sticas b√°sicas del √≠ndice\n",
    "        \n",
    "        Returns:\n",
    "            Dict. Diccionario con estad√≠sticas del √≠ndice\n",
    "        \"\"\"\n",
    "        # Calcular distribuci√≥n de frecuencias\n",
    "        doc_frequencies = [len(doc_set) for doc_set in self.index.values()]\n",
    "        \n",
    "        return {\n",
    "            'total_documents': self.total_documents,\n",
    "            'vocabulary_size': self.vocab_size,\n",
    "            'avg_terms_per_doc': np.mean([len(self.processor.process_text(doc.get_full_text())) \n",
    "                                         for doc in self.documents.values()]),\n",
    "            'max_doc_frequency': max(doc_frequencies) if doc_frequencies else 0,\n",
    "            'min_doc_frequency': min(doc_frequencies) if doc_frequencies else 0\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2573b",
   "metadata": {},
   "source": [
    "## Colecci√≥n de Documentos\n",
    "\n",
    "Utilizamos el dataset Cranfield para probar nuestro sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Colecci√≥n creada con 1400 documentos\n",
      "\n",
      "Primeros 3 documentos:\n",
      "1. Doc[1]: Doc. Title\n",
      "2. Doc[2]: Doc. Title\n",
      "3. Doc[3]: Doc. Title\n"
     ]
    }
   ],
   "source": [
    "def create_documents(dataset) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Crea una colecci√≥n de documentos desde el dataset Cranfield\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset. Dataset de ir_datasets\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]. Lista de documentos de prueba\n",
    "    \"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    for doc in dataset.docs_iter():\n",
    "        # Implementar\n",
    "        pass\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Crear colecci√≥n de documentos\n",
    "sample_docs = create_documents(cranfield_dataset)\n",
    "print(f\"Colecci√≥n creada con {len(sample_docs)} documentos\")\n",
    "print(\"\\nPrimeros 3 documentos:\")\n",
    "for i, doc in enumerate(sample_docs[:3]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44589b99",
   "metadata": {},
   "source": [
    "## Construcci√≥n del √çndice\n",
    "\n",
    "Creamos e indexamos nuestra colecci√≥n de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a43c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando indexaci√≥n de 1400 documentos...\n",
      "   Progreso: 500/1400 documentos procesados\n",
      "   Progreso: 1000/1400 documentos procesados\n",
      "\n",
      "üéâ Indexaci√≥n completada en 24.16 segundos\n",
      "   Documentos: 1400\n",
      "   Vocabulario: 10319 t√©rminos √∫nicos\n"
     ]
    }
   ],
   "source": [
    "# Crear instancia del √≠ndice\n",
    "docusearch = SimpleInvertedIndex()\n",
    "\n",
    "# Construir √≠ndice\n",
    "docusearch.build_index(sample_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31593de9",
   "metadata": {},
   "source": [
    "## Pruebas de B√∫squeda\n",
    "\n",
    "Realizamos b√∫squedas de prueba para validar el funcionamiento del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_search_results(results: List[Tuple[str, Document]], max_content_length: int = 100):\n",
    "    \"\"\"\n",
    "    Muestra los resultados de b√∫squeda de forma legible\n",
    "    \n",
    "    Args:\n",
    "        results: List[Tuple[str, Document]]. Lista de resultados de b√∫squeda\n",
    "        max_content_length: int. Longitud m√°xima del contenido a mostrar\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No se encontraron resultados\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nRESULTADOS DE B√öSQUEDA ({len(results)} documentos)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, (doc_id, doc) in enumerate(results, 1):\n",
    "        content_preview = doc.content[:max_content_length]\n",
    "        if len(doc.content) > max_content_length:\n",
    "            content_preview += \"...\"\n",
    "        \n",
    "        print(f\"{i}. [{doc_id}] {doc.title}\")\n",
    "        print(f\"   Autor: {doc.author} | Categor√≠a: {doc.category} | Fecha: {doc.date}\")\n",
    "        print(f\"   Contenido: {content_preview}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc07643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç B√öSQUEDA 1: 'inteligencia artificial'\n",
      "üîç Buscando t√©rminos: ['university']\n",
      "üìä Encontrados 7 documentos\n",
      "\n",
      "‚úÖ RESULTADOS DE B√öSQUEDA (7 documentos)\n",
      "============================================================\n",
      "1. [220] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: a general purpose analogue correlator for the analysis of\n",
      "random noise signals .\n",
      "a large proportion ...\n",
      "\n",
      "2. [610] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: corner interference effects .\n",
      "  the three-dimensional incompressible flow of fluid along the corner ...\n",
      "\n",
      "3. [549] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: experimental study of the velocity and temperature\n",
      "distribution in a high-velocity vortex-type flow ...\n",
      "\n",
      "4. [344] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: some experimental techniques in mass transfer cooling .\n",
      "  author introduces his survey by a brief re...\n",
      "\n",
      "5. [1229] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: the effect of sweep angle on hypersonic flow over blunt\n",
      "wings .\n",
      "  a series of tests were carried out...\n",
      "\n",
      "6. [830] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: nonlinear deflections of shallow spherical shells .\n",
      "  the equations obtained by chien for the nonlin...\n",
      "\n",
      "7. [1055] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: non-linear bending and buckling of circular plates .\n",
      "  iterative solutions of finite difference\n",
      "appr...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1: B√∫squeda de \"University\"\n",
    "print(\"B√öSQUEDA 1: 'University'\")\n",
    "results1 = docusearch.search(\"university\")\n",
    "show_search_results(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7f45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç B√öSQUEDA 2: 'tecnolog√≠a'\n",
      "üîç Buscando t√©rminos: ['insects']\n",
      "üìä Encontrados 1 documentos\n",
      "\n",
      "‚úÖ RESULTADOS DE B√öSQUEDA (1 documentos)\n",
      "============================================================\n",
      "1. [933] Doc. Title\n",
      "   Autor:  | Categor√≠a: cranfield | Fecha: \n",
      "   Contenido: the characteristics of roughness from insects as observed\n",
      "for two-dimensional, incompressible flow p...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prueba 2: B√∫squeda de \"Insects\"\n",
    "print(\"B√öSQUEDA 2: 'Insects'\")\n",
    "results2 = docusearch.search(\"insects\")\n",
    "show_search_results(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç B√öSQUEDA 3: 'medicina salud'\n",
      "üîç Buscando t√©rminos: ['health']\n",
      "   T√©rmino 'health' no encontrado en el √≠ndice\n",
      "‚ùå No se encontraron resultados\n"
     ]
    }
   ],
   "source": [
    "# Prueba 3: B√∫squeda de \"Health\"\n",
    "print(\"B√öSQUEDA 3: 'Health'\")\n",
    "results3 = docusearch.search(\"health\")\n",
    "show_search_results(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cd2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç B√öSQUEDA 4: 't√©rmino inexistente'\n",
      "üîç Buscando t√©rminos: ['sports']\n",
      "   T√©rmino 'sports' no encontrado en el √≠ndice\n",
      "‚ùå No se encontraron resultados\n"
     ]
    }
   ],
   "source": [
    "# Prueba 4: B√∫squeda sin resultados\n",
    "print(\"B√öSQUEDA 4: 'Sports'\")\n",
    "results4 = docusearch.search(\"Sports\")\n",
    "show_search_results(results4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247017f7-77b0-4c2f-8922-0fb0b955e549",
   "metadata": {},
   "source": [
    "## EJERCICIO ADICIONAL: Implementaci√≥n de Skip Lists\n",
    "\n",
    "### EJERCICIO AVANZADO PARA ESTUDIO INDEPENDIENTE\n",
    "\n",
    "Como ejercicio adicional y para profundizar en estructuras de datos avanzadas para sistemas de b√∫squeda, se propone implementar **Skip Lists** para optimizar las operaciones de intersecci√≥n en las posting lists.\n",
    "\n",
    "#### ¬øQu√© es una Skip List?\n",
    "\n",
    "Una Skip List es una estructura de datos probabil√≠stica que permite realizar operaciones de b√∫squeda, inserci√≥n y eliminaci√≥n en tiempo O(log n) en promedio. En el contexto de √≠ndices invertidos, las skip lists pueden acelerar significativamente las operaciones de intersecci√≥n entre posting lists largas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03402d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EJERCICIOS PR√ÅCTICOS ADICIONALES\n",
    "\n",
    "Los siguientes ejercicios te ayudar√°n a profundizar en conceptos clave de indexaci√≥n y recuperaci√≥n de informaci√≥n. Aplica todo lo que has aprendido sobre preprocesamiento de texto, posting lists y optimizaciones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838c6c0",
   "metadata": {},
   "source": [
    "### EJERCICIO PR√ÅCTICO 1: An√°lisis de Stemming\n",
    "\n",
    "**Situaci√≥n:** Est√°s desarrollando un motor de b√∫squeda en espa√±ol y has notado que el algoritmo de stemming que implementaste est√° agrupando algunas palabras que quiz√°s no deber√≠an estar juntas. \n",
    "\n",
    "Tu algoritmo reduce las siguientes parejas de palabras a la misma forma base:\n",
    "- a. abandono/abandonamiento  \n",
    "- b. absorci√≥n/absorbente\n",
    "- c. mercadeo/mercados\n",
    "- d. universidad/universo\n",
    "- e. volumen/vol√∫menes\n",
    "\n",
    "**Tu tarea:** \n",
    "1. **Analiza cada pareja** y determina cu√°les NO deber√≠an ser conflacionadas (agrupadas) por el stemmer.\n",
    "2. **Justifica tu razonamiento** considerando:\n",
    "   - ¬øTienen las palabras el mismo significado sem√°ntico?\n",
    "   - ¬øUn usuario que busque una palabra estar√≠a interesado en resultados de la otra?\n",
    "   - ¬øQu√© impacto tendr√≠a en la precisi√≥n vs recall del sistema?\n",
    "\n",
    "**Reflexiona:** ¬øC√≥mo podr√≠a este comportamiento del stemmer afectar la experiencia del usuario final en las b√∫squedas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57eccd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EJERCICIO PR√ÅCTICO 2: Optimizaci√≥n de Intersecci√≥n de Posting Lists\n",
    "\n",
    "**Situaci√≥n:** Tu motor de b√∫squeda debe procesar una consulta de dos t√©rminos que requiere intersecci√≥n de posting lists. Has implementado tanto posting lists est√°ndar como posting lists con skip pointers.\n",
    "\n",
    "**Datos del problema:**\n",
    "- **T√©rmino A** tiene una posting list con 16 entradas:  \n",
    "  `[4, 6, 10, 12, 14, 16, 18, 20, 22, 32, 47, 81, 120, 122, 157, 180]`\n",
    "  \n",
    "- **T√©rmino B** tiene una posting list con 1 entrada:  \n",
    "  `[47]`\n",
    "\n",
    "**Tu tarea:**\n",
    "1. **Calcula las comparaciones** necesarias para intersectar estas listas usando:\n",
    "   - a) Posting lists est√°ndar (sin skip pointers)\n",
    "   - b) Posting lists con skip pointers (usando longitud de salto sugerida de ‚àöP, donde P es el tama√±o de la lista)\n",
    "\n",
    "2. **Justifica tus c√°lculos** paso a paso.\n",
    "\n",
    "3. **Analiza el resultado:** \n",
    "   - ¬øCu√°ndo son m√°s efectivos los skip pointers? \n",
    "   - ¬øQu√© pasa cuando una lista es muy peque√±a comparada con la otra?\n",
    "   - ¬øEn qu√© escenarios los skip pointers no ofrecen ventajas?\n",
    "\n",
    "**Bonus:** ¬øC√≥mo cambiar√≠an los c√°lculos si ambas listas tuvieran tama√±os similares y grandes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
